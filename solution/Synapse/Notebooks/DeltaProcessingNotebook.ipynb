{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-14T01:32:32.7631266Z",
              "execution_start_time": "2022-03-14T01:32:32.2347491Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-14T01:30:12.0419856Z",
              "session_id": 10,
              "session_start_time": "2022-03-14T01:30:12.0862879Z",
              "spark_pool": "arkstgsynspads",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(arkstgsynspads, 10, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "SyntaxError",
          "evalue": "EOL while scanning string literal (<stdin>, line 1)",
          "output_type": "error",
          "traceback": [
            "SyntaxError: EOL while scanning string literal (<stdin>, line 1)",
            "  File \"<stdin>\", line 1\n",
            "    TaskObject = '\n",
            "                 ^\n",
            "SyntaxError: EOL while scanning string literal\n"
          ]
        }
      ],
      "source": [
        "TaskObject = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-01T20:42:10.2188104Z",
              "execution_start_time": "2022-03-01T20:42:10.0616085Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-01T20:42:09.9466632Z",
              "session_id": 82,
              "session_start_time": null,
              "spark_pool": "dlzstgsynspads",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(dlzstgsynspads, 82, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: abfss://datalakeraw@dlzstgdlsadsylf2adsl.dfs.core.windows.net/SampleFiles/SalesLT.Customer.chunk_1.parquet\n",
            "Schema: abfss://datalakeraw@dlzstgdlsadsylf2adsl.dfs.core.windows.net/SampleFiles/SalesLT.Customer.json\n",
            "Target: abfss://datalakelanding@dlzstgdlsadsylf2adsl.dfs.core.windows.net/TestOutput/SalesLT-Customer-Delta"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "session_id = random.randint(0,1000000)\n",
        "\n",
        "TaskObjectJson = json.loads(TaskObject)\n",
        "Source = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "Schema = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "Target = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "\n",
        "\n",
        "Source = Source + TaskObjectJson['Source']['RelativePath'] + \"/\" + TaskObjectJson['Source']['DataFileName']\n",
        "Schema = Schema + TaskObjectJson['Source']['RelativePath'] + \"/\" + TaskObjectJson['Source']['SchemaFileName']\n",
        "Target = Target + TaskObjectJson['Target']['RelativePath'] + \"/\" + TaskObjectJson['Target']['DataFileName']\n",
        "\n",
        "\n",
        "#remove any double slashes\n",
        "Source = Source.replace('//', '/')\n",
        "Schema = Schema.replace('//', '/')\n",
        "Target = Target.replace('//', '/')\n",
        "\n",
        "#get source and target data types\n",
        "SourceDT = TaskObjectJson['Source']['Type']\n",
        "TargetDT = TaskObjectJson['Target']['Type']\n",
        "\n",
        "\n",
        "#add abfss\n",
        "Source = \"abfss://\" + Source\n",
        "Schema = \"abfss://\" + Schema\n",
        "Target = \"abfss://\" + Target\n",
        "\n",
        "\n",
        "print (\"Source: \" + Source)\n",
        "print (\"Schema: \" + Schema)\n",
        "print (\"Target: \" + Target)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-01T20:42:29.4180882Z",
              "execution_start_time": "2022-03-01T20:42:15.9281891Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-01T20:42:15.8040424Z",
              "session_id": 82,
              "session_start_time": null,
              "spark_pool": "dlzstgsynspads",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(dlzstgsynspads, 82, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomerID"
          ]
        }
      ],
      "source": [
        "#get scheme file ->  this needs to be made dynamic\n",
        "schema = spark.read.load(Schema, format='json', multiLine=True)\n",
        "#convert it into a list so we can loop it using python rules\n",
        "schema = schema.collect()\n",
        "#loop through each column to find the primary key column\n",
        "for col in schema:\n",
        "    if col.PKEY_COLUMN:\n",
        "        print(col.COLUMN_NAME)\n",
        "        primaryKey = col\n",
        "        break\n",
        "#set up the merge condition used in the next code block\n",
        "mergeCondition = \"oldData.\" + primaryKey.COLUMN_NAME + \" = newData.\" + primaryKey.COLUMN_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-28T21:55:18.8952766Z",
              "execution_start_time": "2022-02-28T21:54:59.2864675Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-28T21:54:59.0347829Z",
              "session_id": 77,
              "session_start_time": null,
              "spark_pool": "dlzstgsynspads",
              "state": "finished",
              "statement_id": 36
            },
            "text/plain": [
              "StatementMeta(dlzstgsynspads, 77, 36, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table already exists. Performing Merge"
          ]
        }
      ],
      "source": [
        "from delta.tables import *\n",
        "if(TargetDT == 'Delta'):\n",
        "    if(SourceDT == 'Parquet'):\n",
        "        print(\"SourceDT = Parquet, TargetDT = Delta.\")\n",
        "        df = spark.read.load(Source, format='parquet')\n",
        "    else:\n",
        "        print(\"SourceDT = Delta, TargetDT = Delta.\")\n",
        "        df = spark.read.load(Source, format='delta')\n",
        "    sql = 'describe detail \"' + Target + '\"'\n",
        "    try:\n",
        "        if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
        "            print(\"Table already exists. Performing Merge\")\n",
        "            olddt = DeltaTable.forPath(spark, Target)  \n",
        "            olddt.alias(\"oldData\").merge(\n",
        "                df.alias(\"newData\"),\n",
        "                mergeCondition) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .execute()\n",
        "        else:\n",
        "            print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
        "            df.write.format(\"delta\").save(Target)\n",
        "    except:\n",
        "        print(\"Table does not exist. Creating new Delta Table.\")    \n",
        "        df.write.format(\"delta\").save(Target)\n",
        "elif(TargetDT == 'Parquet' and SourceDT == 'Delta'):\n",
        "    print(\"SourceDT = Delta, TargetDT = Parquet.\")\n",
        "    df = spark.read.format(\"delta\").load(Source)\n",
        "    df.write.format(\"parquet\").mode(\"overwrite\").save(Target) \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-28T22:09:57.433111Z",
              "execution_start_time": "2022-02-28T22:09:56.3467677Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-28T22:09:56.2196236Z",
              "session_id": 77,
              "session_start_time": null,
              "spark_pool": "dlzstgsynspads",
              "state": "finished",
              "statement_id": 43
            },
            "text/plain": [
              "StatementMeta(dlzstgsynspads, 77, 43, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------------+------+--------+---------+------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|version|          timestamp|userId|userName|operation|                                         operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                                                                                                                                                                                    operationMetrics|\n",
            "+-------+-------------------+------+--------+---------+------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|      9|2022-02-28 21:55:15|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          8|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      8|2022-02-28 21:28:31|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          7|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      7|2022-02-28 21:27:36|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          6|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      6|2022-02-28 21:25:55|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          5|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      5|2022-02-28 06:23:16|  null|    null|    MERGE|[predicate -> (newData.`CustomerID` = oldData.`CustomerID`)]|null|    null|     null|          4|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      4|2022-02-28 06:13:43|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          3|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      3|2022-02-28 03:39:06|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          2|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      2|2022-02-28 03:38:09|  null|    null|    MERGE|[predicate -> (newData.`CustomerID` = oldData.`CustomerID`)]|null|    null|     null|          1|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      1|2022-02-28 03:34:06|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          0|          null|        false|  [numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 1]|\n",
            "|      0|2022-02-28 03:33:05|  null|    null|    WRITE|                  [mode -> ErrorIfExists, partitionBy -> []]|null|    null|     null|       null|          null|         true|                                                                                                                                                      [numFiles -> 1, numOutputBytes -> 89560, numOutputRows -> 847]|\n",
            "+-------+-------------------+------+--------+---------+------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          ]
        }
      ],
      "source": [
        "#olddt.history().show(20, 1000, False)\n",
        "#display(spark.read.format(\"delta\").load(Target))\n",
        "#spark.sql(\"CREATE TABLE SalesLTCustomer USING DELTA LOCATION '{0}'\".format(TargetFile))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-28T22:08:50.983439Z",
              "execution_start_time": "2022-02-28T22:08:50.8269271Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-28T22:08:50.5438093Z",
              "session_id": 77,
              "session_start_time": null,
              "spark_pool": "dlzstgsynspads",
              "state": "finished",
              "statement_id": 41
            },
            "text/plain": [
              "StatementMeta(dlzstgsynspads, 77, 41, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "name 'old_deltaTable' is not defined",
          "output_type": "error",
          "traceback": [
            "NameError: name 'old_deltaTable' is not defined",
            "Traceback (most recent call last):\n",
            "NameError: name 'old_deltaTable' is not defined\n"
          ]
        }
      ],
      "source": [
        "## Execute Upsert\n",
        "#(old_deltaTable\n",
        "# .alias(\"oldData\") \n",
        "# .merge(newIncrementalData.alias(\"newData\"), \"oldData.id = newData.id\")\n",
        "# .whenMatchedUpdate(set = {\"name\": col(\"newData.name\")})\n",
        "# .whenNotMatchedInsert(values = {\"id\": col(\"newData.id\"), \"name\":\n",
        "#                                col(\"newData.name\")})\n",
        "# .execute()\n",
        "#)\n",
        "#\n",
        "# Display the records to check if the records are Merged\n",
        "#display(spark.read.format(\"delta\").load(Target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-01T20:48:15.6259714Z",
              "execution_start_time": "2022-03-01T20:48:13.5022756Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-01T20:48:13.3789395Z",
              "session_id": 82,
              "session_start_time": null,
              "spark_pool": "dlzstgsynspads",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(dlzstgsynspads, 82, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------------+------+--------+---------+------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|version|          timestamp|userId|userName|operation|                                         operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                                                                                                                                                                                    operationMetrics|\n",
            "+-------+-------------------+------+--------+---------+------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|     10|2022-03-01 20:47:52|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          9|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      9|2022-02-28 21:55:15|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          8|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      8|2022-02-28 21:28:31|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          7|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      7|2022-02-28 21:27:36|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          6|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      6|2022-02-28 21:25:55|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          5|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      5|2022-02-28 06:23:16|  null|    null|    MERGE|[predicate -> (newData.`CustomerID` = oldData.`CustomerID`)]|null|    null|     null|          4|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      4|2022-02-28 06:13:43|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          3|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      3|2022-02-28 03:39:06|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          2|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      2|2022-02-28 03:38:09|  null|    null|    MERGE|[predicate -> (newData.`CustomerID` = oldData.`CustomerID`)]|null|    null|     null|          1|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 200]|\n",
            "|      1|2022-02-28 03:34:06|  null|    null|    MERGE|[predicate -> (oldData.`CustomerID` = newData.`CustomerID`)]|null|    null|     null|          0|          null|        false|  [numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 200, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 847, numOutputRows -> 847, numSourceRows -> 847, numTargetFilesRemoved -> 1]|\n",
            "|      0|2022-02-28 03:33:05|  null|    null|    WRITE|                  [mode -> ErrorIfExists, partitionBy -> []]|null|    null|     null|       null|          null|         true|                                                                                                                                                      [numFiles -> 1, numOutputBytes -> 89560, numOutputRows -> 847]|\n",
            "+-------+-------------------+------+--------+---------+------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          ]
        }
      ],
      "source": [
        "#olddt.history().show(20, 1000, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-01T20:48:02.5525496Z",
              "execution_start_time": "2022-03-01T20:47:23.0602668Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-01T20:47:22.8221283Z",
              "session_id": 82,
              "session_start_time": null,
              "spark_pool": "dlzstgsynspads",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(dlzstgsynspads, 82, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"CustomerID\": \"newData.CustomerID\", \"NameStyle\": \"newData.NameStyle\", \"Title\": \"newData.Title\", \"FirstName\": \"newData.FirstName\", \"MiddleName\": \"newData.MiddleName\", \"LastName\": \"newData.LastName\", \"Suffix\": \"newData.Suffix\", \"CompanyName\": \"newData.CompanyName\", \"SalesPerson\": \"newData.SalesPerson\", \"EmailAddress\": \"newData.EmailAddress\", \"Phone\": \"newData.Phone\", \"PasswordHash\": \"newData.PasswordHash\", \"PasswordSalt\": \"newData.PasswordSalt\", \"rowguid\": \"newData.rowguid\", \"ModifiedDate\": \"newData.ModifiedDate\"}\n",
            "{\"CustomerID\": \"newData.CustomerID\", \"NameStyle\": \"newData.NameStyle\", \"Title\": \"newData.Title\", \"FirstName\": \"newData.FirstName\", \"MiddleName\": \"newData.MiddleName\", \"LastName\": \"newData.LastName\", \"Suffix\": \"newData.Suffix\", \"CompanyName\": \"newData.CompanyName\", \"SalesPerson\": \"newData.SalesPerson\", \"EmailAddress\": \"newData.EmailAddress\", \"Phone\": \"newData.Phone\", \"PasswordHash\": \"newData.PasswordHash\", \"PasswordSalt\": \"newData.PasswordSalt\", \"rowguid\": \"newData.rowguid\", \"ModifiedDate\": \"newData.ModifiedDate\"}\n",
            "Table already exists. Performing Merge"
          ]
        }
      ],
      "source": [
        "#########################\n",
        "#NOTE -> This is an alternate way of upserting into delta table. Using manual method of getting each column required from the schema / dataframe and then creating a dictionary to use for the upsert.\n",
        "#           Currently not using this however it does work. Would advise to change the script to just create a dictionary and insert that instead of creating a string to convert into a dict.\n",
        "#########################\n",
        "#from delta.tables import *\n",
        "#df = spark.read.load(Source, format='parquet')\n",
        "#updatecols = []\n",
        "#insertcols = []\n",
        "#for col in schema:\n",
        "#    updatecols.append(col.COLUMN_NAME)\n",
        "#\n",
        "#for col in df.dtypes:\n",
        "#    insertcols.append(col[0])\n",
        "#\n",
        "#creating a string to be converted to dictionary \n",
        "#note -> can easily re-write this as just a dictionary if end up using this method.\n",
        "#updatestring = '{'\n",
        "#insertstring = '{'\n",
        "#\n",
        "#Go through each column in the schema to check what needs to be updated\n",
        "#for col in updatecols:\n",
        "#    updatestring = updatestring + '\"' + col + '\": \"newData.' + col +'\", '\n",
        "#updatestring = updatestring[:-2]\n",
        "#updatestring = updatestring + '}'\n",
        "#\n",
        "#Go through the new data to check what columns need to be inserted\n",
        "#for col in insertcols:\n",
        "#    insertstring = insertstring + '\"' + col + '\": \"newData.' + col +'\", '\n",
        "#insertstring = insertstring[:-2]\n",
        "#insertstring = insertstring + '}'\n",
        "#\n",
        "#print(updatestring)\n",
        "#print(insertstring)\n",
        "#\n",
        "#convert to dict\n",
        "#res = json.loads(updatestring)\n",
        "#res2 = json.loads(insertstring)\n",
        "#\n",
        "#sql = 'describe detail \"' + Target + '\"'\n",
        "#try:\n",
        "#    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
        "#        print(\"Table already exists. Performing Merge\")\n",
        "#        olddt = DeltaTable.forPath(spark, Target)  \n",
        "#        olddt.alias(\"oldData\").merge(\n",
        "#            df.alias(\"newData\"),\n",
        "#            mergeCondition) \\\n",
        "#        .whenMatchedUpdate(set = res) \\\n",
        "#        .whenNotMatchedInsert(values = res2) \\\n",
        "#        .execute()\n",
        "#except:\n",
        "#    print(\"Table does not exist.\")    \n",
        "#    df.write.format(\"delta\").save(Target)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
